{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T09:31:55.949898Z","iopub.execute_input":"2025-08-06T09:31:55.950665Z","iopub.status.idle":"2025-08-06T09:31:56.289723Z","shell.execute_reply.started":"2025-08-06T09:31:55.950627Z","shell.execute_reply":"2025-08-06T09:31:56.288901Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Kaggle Playground Series 2025: Bank Term Deposit Prediction\n\n## Goal\nPredict whether a client will subscribe to a bank term deposit (`y = 1` or `0`).\n\n## Evaluation Metric\n- **ROC AUC**: Area Under the Receiver Operating Characteristic Curve\n\n## Approach\nWe'll follow a structured workflow:\n1. Load and inspect the data\n2. Perform exploratory data analysis (EDA)\n3. Preprocess features\n4. Train a LightGBM model with cross-validation\n5. Engineer high-impact features\n6. Ensemble and submit\n\nLet's get started!","metadata":{}},{"cell_type":"markdown","source":"## 1. Import Libraries\n\nWe start by importing the essential Python libraries needed for this competition:\n\n- **`pandas` & `numpy`**: Data manipulation and numerical operations\n- **`seaborn` & `matplotlib`**: Exploratory Data Analysis (EDA) and visualization\n- **`scikit-learn`**: Preprocessing, model evaluation, and utilities like cross-validation\n- **`lightgbm`**: High-performance gradient boosting for tabular data\n- **`warnings`**: Suppress routine warnings for a cleaner notebook\n\nWe also set default styles for plots to ensure clarity and consistency throughout the analysis.","metadata":{}},{"cell_type":"code","source":"# Basic data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine learning: preprocessing, model selection, metrics\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import roc_auc_score\n\n# Tree-based models (excellent for tabular data)\nimport lightgbm as lgb\n\n# Suppress warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for plots\nsns.set_style(\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (10, 6)\n\nprint(\"‚úÖ Libraries imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T09:31:56.291257Z","iopub.execute_input":"2025-08-06T09:31:56.291777Z","iopub.status.idle":"2025-08-06T09:32:03.189898Z","shell.execute_reply.started":"2025-08-06T09:31:56.291751Z","shell.execute_reply":"2025-08-06T09:32:03.188993Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Load the Dataset\n\nWe begin by loading the training and test datasets provided for the **Playground Series S5E8: Bank Term Deposit Prediction**.\n\n### Objective\n- Load `train.csv` and `test.csv`\n- Inspect basic structure: number of rows, columns, and sample data\n- Prepare for exploratory data analysis (EDA) and modeling\n\nThis competition challenges us to predict whether a client will subscribe (`y = 1`) to a bank's term deposit based on demographic and campaign-related features.","metadata":{}},{"cell_type":"code","source":"# Load datasets\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e8/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e8/test.csv')\n\n# Display shapes\nprint(f\"‚úÖ Training set: {train.shape}\")\nprint(f\"‚úÖ Testing set:  {test.shape}\")\n\n# Show first 5 rows of training data\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T09:32:03.191130Z","iopub.execute_input":"2025-08-06T09:32:03.191758Z","iopub.status.idle":"2025-08-06T09:32:06.314046Z","shell.execute_reply.started":"2025-08-06T09:32:03.191730Z","shell.execute_reply":"2025-08-06T09:32:06.313072Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Quality & Structure Analysis\n\nNow that we‚Äôve loaded the dataset, let‚Äôs inspect:\n- **Data types**: Identify categorical (`object`) vs. numerical features\n- **Missing values**: Even synthetic data may have simulated nulls\n- **Unique values in categorical features**: Look for high cardinality or unusual categories like `\"unknown\"`\n- **Statistical summary**: Detect outliers or extreme values (e.g., in `duration`, `balance`)\n- **Target distribution**: Check for class imbalance\n\nThis step ensures we clean and prepare the data correctly before modeling.","metadata":{}},{"cell_type":"code","source":"# === 1. Data Types ===\nprint(\"üìÅ Data Types of Features:\")\nprint(train.dtypes)\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# === 2. Missing Values ===\nprint(\"üß© Missing Values (Training Set):\")\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nif len(missing) == 0:\n    print(\"‚úÖ No missing values detected.\")\nelse:\n    print(missing)\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# === 3. Unique Values for Categorical Columns (Safely) ===\ncategorical_cols = train.select_dtypes(include='object').columns.tolist()\n\nprint(\"üè∑Ô∏è  Unique Values in Categorical Features:\")\nfor col in categorical_cols:\n    unique_vals = sorted(train[col].astype(str).unique())\n    print(f\"{col}: {train[col].nunique()} unique values ‚Üí {unique_vals}\")\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# === 4. Numerical Summary Statistics ===\nnumerical_cols = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\nif 'id' in numerical_cols:\n    numerical_cols.remove('id')  # Remove 'id' from numerical features if present\nif 'y' in numerical_cols:\n    numerical_cols.remove('y')   # Keep target separate\n\nprint(\"üìà Summary Statistics (Numerical Features - Excluding id & y):\")\ndisplay(train[numerical_cols].describe().T)\n\n# === 5. Target Distribution ===\nprint(\"üéØ Target Distribution (y):\")\ntarget_counts = train['y'].value_counts().sort_index()\nprint(target_counts)\nprint(\"\\nPercentage of '1's (subscribed): {:.2f}%\".format(100 * train['y'].mean()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T09:32:06.316056Z","iopub.execute_input":"2025-08-06T09:32:06.316323Z","iopub.status.idle":"2025-08-06T09:32:07.649437Z","shell.execute_reply.started":"2025-08-06T09:32:06.316301Z","shell.execute_reply":"2025-08-06T09:32:07.648541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Exploratory Data Analysis (EDA)\n\nLet‚Äôs visualize:\n- **Target distribution**: How imbalanced is the subscription rate?\n- **Numerical features vs. target**: Do high `balance` or long `duration` lead to more subscriptions?\n- **Categorical features**: Which job, contact method, or month performs best?\n\nVisual insights will guide feature engineering and modeling decisions.","metadata":{}},{"cell_type":"code","source":"# Set up figure styling\nsns.set_style(\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (12, 8)\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# === 1. Target Distribution ===\naxes[0,0].pie(train['y'].value_counts(), labels=['No (0)', 'Yes (1)'], autopct='%1.1f%%', colors=['#ff9999','#66b3ff'])\naxes[0,0].set_title(\"Target Distribution (y): Subscription Rate\", fontsize=14)\n\n# === 2. Age vs. y ===\nsns.boxplot(data=train, x='y', y='age', ax=axes[0,1])\naxes[0,1].set_title(\"Age vs. Subscription (y)\")\n\n# === 3. Balance vs. y ===\nsns.boxplot(data=train, x='y', y='balance', ax=axes[1,0])\naxes[1,0].set_title(\"Balance vs. Subscription (y)\")\n\n# === 4. Duration vs. y ===\nsns.boxplot(data=train, x='y', y='duration', ax=axes[1,1])\naxes[1,1].set_title(\"Call Duration vs. Subscription (y)\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T09:32:07.650258Z","iopub.execute_input":"2025-08-06T09:32:07.650490Z","iopub.status.idle":"2025-08-06T09:32:09.292800Z","shell.execute_reply.started":"2025-08-06T09:32:07.650469Z","shell.execute_reply":"2025-08-06T09:32:09.291792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Categorical Feature Analysis\n\nCategorical features often contain rich information. Let‚Äôs explore:\n- Which `job` types are most likely to subscribe?\n- Does `education` level affect subscription rates?\n- How does `contact` method influence outcomes?\n- What‚Äôs the impact of `poutcome` (previous campaign outcome)?\n\nThese insights will guide feature encoding and engineering.","metadata":{}},{"cell_type":"code","source":"# Set up figure styling\nsns.set_style(\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (15, 10)\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# === 1. Job vs. Subscription Rate ===\njob_subscription_rate = train.groupby('job')['y'].mean().sort_values()\nsns.barplot(x=job_subscription_rate.values, y=job_subscription_rate.index, ax=axes[0,0], palette='viridis')\naxes[0,0].set_title(\"Subscription Rate by Job\", fontsize=14)\naxes[0,0].set_xlabel(\"Subscription Rate\")\n\n# === 2. Education vs. Subscription Rate ===\nedu_subscription_rate = train.groupby('education')['y'].mean().sort_values()\nsns.barplot(x=edu_subscription_rate.values, y=edu_subscription_rate.index, ax=axes[0,1], palette='viridis')\naxes[0,1].set_title(\"Subscription Rate by Education Level\", fontsize=14)\naxes[0,1].set_xlabel(\"Subscription Rate\")\n\n# === 3. Contact Method vs. Subscription Rate ===\ncontact_subscription_rate = train.groupby('contact')['y'].mean().sort_values()\nsns.barplot(x=contact_subscription_rate.values, y=contact_subscription_rate.index, ax=axes[1,0], palette='viridis')\naxes[1,0].set_title(\"Subscription Rate by Contact Method\", fontsize=14)\naxes[1,0].set_xlabel(\"Subscription Rate\")\n\n# === 4. Previous Campaign Outcome vs. Subscription Rate ===\npoutcome_subscription_rate = train.groupby('poutcome')['y'].mean().sort_values()\nsns.barplot(x=poutcome_subscription_rate.values, y=poutcome_subscription_rate.index, ax=axes[1,1], palette='viridis')\naxes[1,1].set_title(\"Subscription Rate by Previous Campaign Outcome\", fontsize=14)\naxes[1,1].set_xlabel(\"Subscription Rate\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T09:32:09.293720Z","iopub.execute_input":"2025-08-06T09:32:09.294016Z","iopub.status.idle":"2025-08-06T09:32:10.608492Z","shell.execute_reply.started":"2025-08-06T09:32:09.293983Z","shell.execute_reply":"2025-08-06T09:32:10.607442Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Feature Engineering & Preprocessing\n\nNow that we‚Äôve explored the data, let‚Äôs prepare it for modeling:\n\n1. **Handle Categorical Variables**:\n   - Use `OrdinalEncoder` for ordered features like `education`, including `'unknown'`.\n   - Use `OneHotEncoder` for unordered categories like `job`, `contact`, etc.\n\n2. **Feature Engineering**:\n   - Create interaction features (e.g., `balance_per_campaign`)\n   - Consider binned or transformed versions of numerical features\n\n3. **Robust Preprocessing**:\n   - Ensure the pipeline handles unseen categories gracefully\n\nLet‚Äôs build a clean, reliable preprocessing pipeline.","metadata":{}},{"cell_type":"code","source":"# Import libraries\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\nprint(\"üîÑ Starting preprocessing...\")\n\n# === 1. Feature Engineering (Do this FIRST on train and test) ===\ntrain = train.copy()\ntest = test.copy()\n\n# New interaction features\ntrain['balance_per_campaign'] = train['balance'] / (train['campaign'] + 1)\ntest['balance_per_campaign'] = test['balance'] / (test['campaign'] + 1)\n\ntrain['duration_per_campaign'] = train['duration'] / (train['campaign'] + 1)\ntest['duration_per_campaign'] = test['duration'] / (test['campaign'] + 1)\n\n# Log transform highly skewed features\ntrain['log_balance'] = np.log1p(train['balance'])\ntest['log_balance'] = np.log1p(test['balance'])\n\n# Bin age into groups (optional)\ntrain['age_bin'] = pd.cut(train['age'], bins=5, labels=False)\ntest['age_bin'] = pd.cut(test['age'], bins=5, labels=False)\n\n# Update lists of features\nnumerical_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous',\n                  'balance_per_campaign', 'duration_per_campaign', 'log_balance', 'age_bin']\n\ncategorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n\n# Remove 'education' from one-hot list since we handle it separately\nonehot_features = [col for col in categorical_cols if col != 'education']\n\n# === 2. Define Encoders ===\n# Ordinal Encoder for 'education' ‚Äî now includes 'unknown'\nordinal_encoder = OrdinalEncoder(\n    categories=[['primary', 'secondary', 'tertiary', 'unknown']],\n    handle_unknown='use_encoded_value',\n    unknown_value=-1\n)\n\n# One-Hot Encoder for other categoricals\nonehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n\n# === 3. Build ColumnTransformer (Only Once!) ===\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', numerical_cols),           # All numerical features\n        ('ord', ordinal_encoder, ['education']),           # Ordinal encode education\n        ('cat', onehot_encoder, onehot_features)          # One-hot rest\n    ],\n    remainder='drop'  # Drop any unlisted columns\n)\n\n# === 4. Prepare X and y ===\nX_train = train.drop(['id', 'y'], axis=1)\ny_train = train['y']\nX_test = test.drop('id', axis=1)\n\n# === 5. Fit and Transform ===\nX_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n\nprint(\"‚úÖ Preprocessing complete!\")\nprint(\"Training shape after preprocessing:\", X_train_processed.shape)\nprint(\"Test shape after preprocessing:\", X_test_processed.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T09:32:10.609905Z","iopub.execute_input":"2025-08-06T09:32:10.610586Z","iopub.status.idle":"2025-08-06T09:32:14.112088Z","shell.execute_reply.started":"2025-08-06T09:32:10.610556Z","shell.execute_reply":"2025-08-06T09:32:14.111189Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Model Training: LightGBM with Cross-Validation\n\nWe'll train a **LightGBM** model using:\n- **Stratified K-Fold CV** (5 folds) ‚Üí ensures balanced splits\n- **Early stopping** ‚Üí prevents overfitting\n- **OOF (Out-of-Fold) predictions** ‚Üí reliable CV score\n- **Test averaging** ‚Üí more stable submission\n\nLightGBM excels on tabular data and handles our encoded features perfectly.","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\n\n# === 1. Set up cross-validation ===\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# === 2. LightGBM Parameters ===\nlgb_params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.8,\n    'baging_fraction': 0.8,\n    'lambda_l1': 1.0,\n    'lambda_l2': 1.0,\n    'min_child_samples': 20,\n    'verbose': -1,\n    'random_state': 42\n}\n\n# === 3. Training setup ===\noof_preds = np.zeros(X_train_processed.shape[0])\ntest_preds = np.zeros(X_test_processed.shape[0])\ncv_scores = []\nbest_iterations = []  # ‚úÖ Store best iteration from each fold\n\nprint(\"üöÄ Starting Cross-Validation...\\n\")\n\n# === 4. Train one fold at a time ===\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_train_processed, y_train)):\n    print(f\"üèãÔ∏è‚Äç‚ôÇÔ∏è Training Fold {fold + 1}/{n_splits}\")\n    \n    X_tr, X_val = X_train_processed[train_idx], X_train_processed[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    train_set = lgb.Dataset(X_tr, label=y_tr)\n    val_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n    \n    model = lgb.train(\n        params=lgb_params,\n        train_set=train_set,\n        num_boost_round=10000,\n        valid_sets=[train_set, val_set],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50, verbose=True),\n            lgb.log_evaluation(100)\n        ]\n    )\n    \n    # Save predictions and score\n    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n    oof_preds[val_idx] = val_pred\n    \n    fold_auc = roc_auc_score(y_val, val_pred)\n    cv_scores.append(fold_auc)\n    best_iterations.append(model.best_iteration)  # ‚úÖ Save best iteration\n    \n    print(f\"‚úÖ Fold {fold + 1} AUC: {fold_auc:.6f}, Best Iteration: {model.best_iteration}\\n\")\n\n# === 5. Final CV Score ===\nmean_cv_auc = np.mean(cv_scores)\nstd_cv_auc = np.std(cv_scores)\nprint(f\"üéâ Mean CV AUC: {mean_cv_auc:.6f} ¬± {std_cv_auc:.6f}\")\nprint(f\"üìå OOF ROC AUC: {roc_auc_score(y_train, oof_preds):.6f}\")\nprint(f\"üìä Average Best Iteration: {int(np.mean(best_iterations))}\")\n\n# === 6. Predict on Test Set ===\nprint(\"\\nüîÆ Generating final test predictions...\")\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_train_processed, y_train)):\n    X_tr, y_tr = X_train_processed[train_idx], y_train.iloc[train_idx]\n    \n    train_set = lgb.Dataset(X_tr, label=y_tr)\n    \n    # Use stored best iteration from this fold\n    num_rounds = best_iterations[fold]\n    \n    model = lgb.train(\n        params=lgb_params,\n        train_set=train_set,\n        num_boost_round=num_rounds,\n        callbacks=[\n            lgb.log_evaluation(False)  # Silent\n        ]\n    )\n    \n    test_preds += model.predict(X_test_processed, num_iteration=num_rounds) / n_splits\n\nprint(\"‚úÖ Final test predictions ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T09:32:14.112935Z","iopub.execute_input":"2025-08-06T09:32:14.113227Z","iopub.status.idle":"2025-08-06T10:02:53.849586Z","shell.execute_reply.started":"2025-08-06T09:32:14.113200Z","shell.execute_reply":"2025-08-06T10:02:53.847566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Create Submission File\n\nFinal step: save predictions!\n","metadata":{}},{"cell_type":"code","source":"# Create submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'y': test_preds  # Already averaged across 5 folds\n})\n\n# Save to CSV\nsubmission.to_csv('submission.csv', index=False)\n\n# Display first few rows\nprint(\"üéâ Submission file created: 'submission.csv'\")\nprint(f\"üìä Shape: {submission.shape}\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T10:02:53.851036Z","iopub.execute_input":"2025-08-06T10:02:53.851323Z","iopub.status.idle":"2025-08-06T10:02:54.483877Z","shell.execute_reply.started":"2025-08-06T10:02:53.851300Z","shell.execute_reply":"2025-08-06T10:02:54.482846Z"}},"outputs":[],"execution_count":null}]}